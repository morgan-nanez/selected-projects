{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VSjS_XAbjQI"
   },
   "source": [
    "# Notebook for Programming Question 2\n",
    "Welcome to the programming portion of the assignment! Each assignment throughout the semester will have a written portion and a programming portion. We will be using [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true), so if you have never used it before, take a quick look through this introduction: [Working with Google Colab](https://docs.google.com/document/d/1LlnXoOblXwW3YX-0yG_5seTXJsb3kRdMMRYqs8Qqum4/edit?usp=sharing).\n",
    "\n",
    "We'll also be programming in Python, which we will assume a basic familiarity with. Python has fantastic community support and we'll be using numerous packages for machine learning (ML) and natural language processing (NLP) tasks.\n",
    "\n",
    "### Learning Objectives\n",
    "In this problem we will implement logistic regression and test it on a sentiment analysis dataset.\n",
    "\n",
    "### Writing Code\n",
    "Look for the keyword \"TODO\" and fill in your code in the empty space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AslCbBsMbol6"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOV13zuwihm6"
   },
   "source": [
    "#### Class and function for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "dlf4P1apdf-w"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# Define a class to store a single sentiment example\n",
    "class SentimentExample:\n",
    "    def __init__(self, words, label):\n",
    "        self.words = words\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.words) + \"; label=\" + repr(self.label)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "# Reads sentiment examples in the format [0 or 1]<TAB>[raw sentence]; tokenizes and cleans the sentences.\n",
    "def read_sentiment_examples(infile):\n",
    "    f = open(infile, encoding='iso8859')\n",
    "    exs = []\n",
    "    for line in f:\n",
    "            fields = line.strip().split(\" \")\n",
    "            label = 0 if \"0\" in fields[0] else 1\n",
    "            exs.append(SentimentExample(fields[1:], label))\n",
    "    f.close()\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "manEehMeimHD"
   },
   "source": [
    "#### Download and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2cvJYNyjlOy",
    "outputId": "8026b2c2-41dd-4ca1-e11e-3e8122810b08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  721k  100  721k    0     0  3052k      0 --:--:-- --:--:-- --:--:-- 3096k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 94400  100 94400    0     0   756k      0 --:--:-- --:--:-- --:--:--  794k\n"
     ]
    }
   ],
   "source": [
    "!curl -O  https://princeton-nlp.github.io/cos484/assignments/a1/train.txt\n",
    "!curl -O  https://princeton-nlp.github.io/cos484/assignments/a1/dev.txt\n",
    "train_file = 'train.txt'\n",
    "dev_file = 'dev.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulsaBn24p-LX"
   },
   "source": [
    "#### Indexer for examples\n",
    "This section contains code for an Indexer which is useful for creating a mapping between words and indices. It has already been implemented for you. Do read it and try to understand what the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "Ck1X_FzPqMVk"
   },
   "outputs": [],
   "source": [
    "# Bijection between objects and integers starting at 0. Useful for mapping\n",
    "# labels, features, etc. into coordinates of a vector space.\n",
    "\n",
    "# This class creates a mapping between objects (here words) and unique indices\n",
    "# For example: apple->1, banana->2, and so on\n",
    "class Indexer(object):\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in range(0, len(self))])\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "\n",
    "    # Returns the object corresponding to the particular index\n",
    "    def get_object(self, index):\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "\n",
    "    def contains(self, object):\n",
    "        return self.index_of(object) != -1\n",
    "\n",
    "    # Returns -1 if the object isn't present, index otherwise\n",
    "    def index_of(self, object):\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "\n",
    "    # Adds the object to the index if it isn't present, always returns a nonnegative index\n",
    "    def add_and_get_index(self, object, add=True):\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIN8X-vpkeBh",
    "outputId": "1327dacd-5783-456d-e02a-2157a41ab0b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920 train examples: 3610 positive, 3310 negative\n",
      "872 dev examples\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the files\n",
    "train_exs = read_sentiment_examples(train_file)\n",
    "dev_exs = read_sentiment_examples(dev_file)\n",
    "n_pos = 0\n",
    "n_neg = 0\n",
    "for ex in train_exs:\n",
    "    if ex.label == 1:\n",
    "        n_pos += 1\n",
    "    else:\n",
    "        n_neg += 1\n",
    "print(\"%d train examples: %d positive, %d negative\" % (len(train_exs), n_pos, n_neg))\n",
    "print(\"%d dev examples\" % len(dev_exs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there', 'is', 'a', 'freedom', 'to', 'watching', 'stunts', 'that', 'are', 'this', 'crude', ',', 'this', 'fast-paced', 'and', 'this', 'insane', '.']; label=1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_exs[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaC07CdvlB-9"
   },
   "source": [
    "### Define Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "u0b4C1DImvuM"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSCzfcnMjm-r"
   },
   "source": [
    "#### Define feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "_eGn0zEOnLOq"
   },
   "outputs": [],
   "source": [
    "# Feature extraction base type. Takes an example and returns an indexed list of features.\n",
    "class FeatureExtractor(object):\n",
    "    # Extract features. Includes a flag add_to_indexer to control whether the indexer should be expanded.\n",
    "    # At test time, any unseen features should be discarded, but at train time, we probably want to keep growing it.\n",
    "    def extract_features(self, ex, add_to_indexer):\n",
    "        raise Exception(\"Don't call me, call my subclasses\")\n",
    "\n",
    "\n",
    "# Extracts unigram bag-of-words features from a sentence. It's up to you to decide how you want to handle counts\n",
    "class UnigramFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, indexer: Indexer):\n",
    "        self.indexer = indexer\n",
    "\n",
    "    def extract_features(self, ex, add_to_indexer=False):\n",
    "        features = Counter()\n",
    "        for w in ex.words:\n",
    "            feat_idx = self.indexer.add_and_get_index(w) \\\n",
    "                if add_to_indexer else self.indexer.index_of(w)\n",
    "            if feat_idx != -1:\n",
    "                features[feat_idx] += 1.0\n",
    "        return features\n",
    "\n",
    "\n",
    "# Bigram feature extractor analogous to the unigram one.\n",
    "class BigramFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, indexer: Indexer):\n",
    "        self.indexer = indexer\n",
    "\n",
    "    def extract_features(self, ex, add_to_indexer=False):\n",
    "        features = Counter()\n",
    "        for i in range(len(ex.words) - 1):\n",
    "            w = ex.words[i] + \"||\" + ex.words[i + 1]\n",
    "            feat_idx = self.indexer.add_and_get_index(w) \\\n",
    "                if add_to_indexer else self.indexer.index_of(w)\n",
    "            if feat_idx != -1:\n",
    "                features[feat_idx] += 1.0\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLEP8hSXnvP5"
   },
   "source": [
    "#### Define base classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "7CgFsVR4nyGN"
   },
   "outputs": [],
   "source": [
    "# Sentiment classifier base type\n",
    "class SentimentClassifier(object):\n",
    "    # Makes a prediction for the given\n",
    "    def predict(self, ex: SentimentExample):\n",
    "        raise Exception(\"Don't call me, call my subclasses\")\n",
    "\n",
    "# Always predicts the positive class\n",
    "class AlwaysPositiveClassifier(SentimentClassifier):\n",
    "    def predict(self, ex: SentimentExample):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1188585304.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[77], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    old train :\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "old train :         \n",
    "      \n",
    "        # STEP 1: Write a 'for' loop which iterates over the dataset num_iters times\n",
    "        for iter in range(num_iters):\n",
    "        \n",
    "        # STEP 2: Write an inner 'for' loop for each step of gradient descent\n",
    "        # You can use stochastic gradient descent or mini-batch SGD\n",
    "            batch = np.random.random_integers(0, len(train_examples)-1,1000)\n",
    "            for b in batch:\n",
    "                train_ex = train_examples[b]\n",
    "                y_prime = self.predict(train_ex, test=False)\n",
    "                y = train_ex.label\n",
    "\n",
    "                one_hot = np.zeros(self.corpus_length)\n",
    "                input_feature = self.feat_extractor.extract_features(train_ex, add_to_indexer =True)\n",
    "                input_feature_indexes = np.asarray(list(input_feature.keys()))\n",
    "                one_hot[input_feature_indexes] = 1#np.asarray(list(input_feature.values()))\n",
    "\n",
    "              #  print(one_hot)\n",
    "\n",
    "              \n",
    "                dw =  np.dot(one_hot.T, y_prime-y) + reg_lambda**2\n",
    "                db =  y_prime-y\n",
    "\n",
    "\n",
    "                #print(y_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKEB5sYeodoh"
   },
   "source": [
    "#### Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "Kcp4Wkbqoiuu"
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(SentimentClassifier):\n",
    "    def __init__(self, feat_extractor: FeatureExtractor, train_examples, num_iters=50, reg_lambda=0.0, learning_rate=0.1):\n",
    "        # TODO: Initialize the logistic regression model\n",
    "        \n",
    "        # Arguments: feat_extractor is unigram or bigram, train_examples is train dataset\n",
    "        # num_iters is the number of epochs, reg_lambda is the regularization parameter\n",
    "        # learning_rate is the learning rate used in gradient descent\n",
    "        \n",
    "        # STEP 1: Define variables for weights and biases, and initialize them\n",
    "        self.corpus_length = len(feat_extractor.indexer)\n",
    "        self.weights = np.ones(self.corpus_length)*0.5 #size of vocabulary\n",
    "        self.bias = np.ones(self.corpus_length)*0.5\n",
    "        self.feat_extractor = feat_extractor\n",
    "\n",
    "        \n",
    "        # STEP 2: Call the train() function. (This has already been done for you)\n",
    "\n",
    "        ##### SOLUTION START #####\n",
    "\n",
    "\n",
    "        ##### SOLUTION END #####\n",
    "\n",
    "        self.train(feat_extractor, train_examples, num_iters, reg_lambda, learning_rate)\n",
    "\n",
    "    def train(self, feat_extractor: FeatureExtractor, train_examples, num_iters=50, reg_lambda=0.0, learning_rate=0.1):\n",
    "        # TODO: Function for training the logistic regression mode\n",
    "\n",
    "\n",
    "        for iter in range(num_iters):\n",
    "        # Use mini-batch SGD\n",
    "            batch_size = 10\n",
    "            num_batches = len(train_examples) // batch_size\n",
    "    \n",
    "            for _ in range(num_batches):\n",
    "                # Randomly select a batch\n",
    "                batch_indices = np.random.choice(len(train_examples), size=batch_size, replace=False)\n",
    "                batch_examples = [train_examples[i] for i in batch_indices]\n",
    "    \n",
    "                # Initialize gradients\n",
    "                dw = np.zeros(self.corpus_length)\n",
    "                db = 0.0\n",
    "    \n",
    "                for train_ex in batch_examples:\n",
    "                    y_prime = self.predict(train_ex, test=False)\n",
    "                    y = train_ex.label\n",
    "    \n",
    "                    one_hot = np.zeros(self.corpus_length)\n",
    "                    input_feature = feat_extractor.extract_features(train_ex, add_to_indexer=True)\n",
    "                    input_feature_indexes = np.asarray(list(input_feature.keys()))\n",
    "                    one_hot[input_feature_indexes] = 1\n",
    "    \n",
    "                    # Compute gradients\n",
    "                    dw += np.dot(one_hot, y_prime - y) + reg_lambda * self.weights\n",
    "                    db += y_prime - y\n",
    "    \n",
    "                # Update weights and biases using the mean of the gradients\n",
    "                self.weights -= learning_rate * (dw / batch_size)\n",
    "                self.bias -= learning_rate * (db / batch_size)\n",
    "\n",
    "     \n",
    "\n",
    "        # You can use stochastic gradient descent or mini-batch SGD\n",
    "        \n",
    "        # STEP 3: In each step of gradient descent apply the update rule\n",
    "        # to weights and biases\n",
    "                self.weights -= learning_rate*dw\n",
    "                self.bias -= learning_rate*db  \n",
    "\n",
    "        ##### SOLUTION START #####\n",
    "\n",
    "        ##### SOLUTION END #####        \n",
    "\n",
    "    def predict(self, ex, test=True):\n",
    "        # TODO: Logistic regression model's prediction for a single example\n",
    "        \n",
    "        ##### SOLUTION START #####\n",
    "\n",
    "        one_hot = np.zeros(self.corpus_length)\n",
    "        input_feature = self.feat_extractor.extract_features(ex)\n",
    "        input_feature_indexes = np.asarray(list(input_feature.keys()))\n",
    "        if len(input_feature_indexes) < 1:\n",
    "            return 0\n",
    "            \n",
    "        one_hot[input_feature_indexes] = 1#np.asarray(list(input_feature.values()))\n",
    "\n",
    "        #compute wTx\n",
    "        pre_sig = np.dot(one_hot, self.weights) + self.bias\n",
    "       # print(pre_sig.shape)\n",
    "       # print(pre_sig)\n",
    "        t = np.mean(1 /(1+np.exp(-pre_sig)))\n",
    "        #print(np.mean(t))\n",
    "\n",
    "        if test:\n",
    "            return 1 if t >= 0.5 else 0\n",
    "        else:\n",
    "            return t\n",
    "\n",
    "        #retur sigmoid(wt*x + b)\n",
    "        #return np.mean(1 / (1+np.exp(-pre_sig)))\n",
    "\n",
    "\n",
    "\n",
    "        ##### SOLUTION END #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "mjqSmMtGZdxF"
   },
   "outputs": [],
   "source": [
    "#unigram model should be high for 70 (dev) 99 for train\n",
    "#bigram model should be higher than 60,, maybe 75?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cesGHOCipwgL"
   },
   "source": [
    "#### Training function for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "NLwZpsU3qC31"
   },
   "outputs": [],
   "source": [
    "# Train a logsitic regression model on the given training examples using the given FeatureExtractor\n",
    "def train_lr(train_exs: List[SentimentExample], feat_extractor: FeatureExtractor, reg_lambda, learning_rate = 0.1) -> LogisticRegressionClassifier:\n",
    "    # TODO: Function for training logistic regression model.\n",
    "    # Populate the feature_extractor.\n",
    "    # Initialize and return an object of instance LogisticRegressionClassifier\n",
    "    \n",
    "    ##### SOLUTION START #####\n",
    "\n",
    "    #populate feature extractor\n",
    "    for train in train_exs:\n",
    "        feat_extractor.extract_features(train, add_to_indexer=True)\n",
    "\n",
    "    # Initialize and return an object of instance LogisticRegressionClassifier\n",
    "    log_classifier = LogisticRegressionClassifier(feat_extractor, train_exs, learning_rate = learning_rate)\n",
    "    return log_classifier\n",
    "\n",
    "\n",
    "    ##### SOLUTION END #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "xMHrP9X7qItd"
   },
   "outputs": [],
   "source": [
    "# Main entry point for your modifications. Trains and returns one of several models depending on the options passed\n",
    "def train_model(feature_type, model_type, train_exs, reg_lambda=0.0, learning_rate = 0.1):\n",
    "    print(reg_lambda)\n",
    "    # Initialize feature extractor\n",
    "    if feature_type == \"unigram\":\n",
    "        # Add additional preprocessing code here\n",
    "        feat_extractor = UnigramFeatureExtractor(Indexer())\n",
    "    elif feature_type == \"bigram\":\n",
    "        # Add additional preprocessing code here\n",
    "        feat_extractor = BigramFeatureExtractor(Indexer())\n",
    "    else:\n",
    "        raise Exception(\"Pass unigram or bigram\")\n",
    "\n",
    "    # Train the model\n",
    "    if model_type == \"AlwaysPositive\":\n",
    "        model = AlwaysPositiveClassifier()\n",
    "    elif model_type == \"LogisticRegression\":\n",
    "        model = train_lr(train_exs, feat_extractor, reg_lambda=reg_lambda)\n",
    "    else:\n",
    "        raise Exception(\"Pass AlwaysPositive or LogisticRegression\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO3EqbavuJpz"
   },
   "source": [
    "### Functions for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "mgTqmbGcuM3f"
   },
   "outputs": [],
   "source": [
    "# Evaluates a given classifier on the given examples\n",
    "def evaluate(classifier, exs):\n",
    "    #print(classifier, exs)\n",
    "    return print_evaluation([ex.label for ex in exs], [classifier.predict(ex) for ex in exs])\n",
    "\n",
    "\n",
    "# Prints accuracy comparing golds and predictions, each of which is a sequence of 0/1 labels.\n",
    "def print_evaluation(golds, predictions):\n",
    "    num_correct = 0\n",
    "    num_pos_correct = 0\n",
    "    num_pred = 0\n",
    "    num_gold = 0\n",
    "    num_total = 0\n",
    "    if len(golds) != len(predictions):\n",
    "        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" %\n",
    "                        (len(golds), len(predictions)))\n",
    "    for idx in range(0, len(golds)):\n",
    "        gold = golds[idx]\n",
    "        prediction = predictions[idx]\n",
    "        if prediction == gold:\n",
    "            num_correct += 1\n",
    "        if prediction == 1:\n",
    "            num_pred += 1\n",
    "        if gold == 1:\n",
    "            num_gold += 1\n",
    "        if prediction == 1 and gold == 1:\n",
    "            num_pos_correct += 1\n",
    "        num_total += 1\n",
    "\n",
    "    print(\"Accuracy: %i / %i = %.2f %%\" %\n",
    "          (num_correct, num_total,\n",
    "           num_correct * 100.0 / num_total))\n",
    "    return num_correct * 100.0 / num_total\n",
    "    \n",
    "# Evaluate on train and dev dataset\n",
    "def eval_train_dev(model):\n",
    "    print(\"===== Train Accuracy =====\")\n",
    "    train_acc = evaluate(model, train_exs)\n",
    "    print(\"===== Dev Accuracy =====\")\n",
    "    eval_acc = evaluate(model, dev_exs)\n",
    "    return [train_acc, eval_acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwrQ1_lHv-Co"
   },
   "source": [
    "### Unigram vs Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1KNbz32wwws",
    "outputId": "acfebd0f-fc7d-40af-a828-ca10d1864db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6899 / 6920 = 99.70 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 679 / 872 = 77.87 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[99.69653179190752, 77.86697247706422]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate logistic regression with unigram features\n",
    "unigram_model = train_model('unigram', 'LogisticRegression', train_exs)\n",
    "eval_train_dev(unigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "wF2xxlx2roLC",
    "outputId": "dead2bf2-7589-4427-e7d3-d6d5ab9b2865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6920 / 6920 = 100.00 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 560 / 872 = 64.22 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[100.0, 64.22018348623853]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate logistic regression with bigram features\n",
    "unigram_model = train_model('bigram', 'LogisticRegression', train_exs)\n",
    "eval_train_dev(unigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response 2a\n",
    "Unigram and bigram accuracies printed above. I found that both tested very high for the trainng set, leading me to believe there may be some overfitting at hand. I found that, on the test set, that unigram preformed higher on the unigram model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jnCD2esA5Aa"
   },
   "source": [
    "### Logistic regression with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgSNsC5kA9Gs",
    "outputId": "9a99aa75-d5fc-4522-f858-689393056c5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6890 / 6920 = 99.57 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 671 / 872 = 76.95 %\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6920 / 6920 = 100.00 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 555 / 872 = 63.65 %\n",
      "0.01\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6901 / 6920 = 99.73 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 672 / 872 = 77.06 %\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6920 / 6920 = 100.00 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 556 / 872 = 63.76 %\n",
      "0.05\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6887 / 6920 = 99.52 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 674 / 872 = 77.29 %\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6920 / 6920 = 100.00 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 556 / 872 = 63.76 %\n",
      "0.1\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6897 / 6920 = 99.67 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 672 / 872 = 77.06 %\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6920 / 6920 = 100.00 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 559 / 872 = 64.11 %\n",
      "0.5\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6892 / 6920 = 99.60 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 674 / 872 = 77.29 %\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6920 / 6920 = 100.00 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 559 / 872 = 64.11 %\n",
      "1\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6902 / 6920 = 99.74 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 672 / 872 = 77.06 %\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6920 / 6920 = 100.00 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 559 / 872 = 64.11 %\n",
      "10\n",
      "0.1\n",
      "===== Train Accuracy =====\n",
      "Accuracy: 6897 / 6920 = 99.67 %\n",
      "===== Dev Accuracy =====\n",
      "Accuracy: 674 / 872 = 77.29 %\n",
      "0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m uni_train_accs\u001b[38;5;241m.\u001b[39mappend(train_acc)\n\u001b[1;32m     18\u001b[0m uni_dev_accs\u001b[38;5;241m.\u001b[39mappend(dev_acc)\n\u001b[0;32m---> 20\u001b[0m bigram_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbigram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLogisticRegression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_exs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m train_acc, dev_acc \u001b[38;5;241m=\u001b[39m eval_train_dev(bigram_model)\n\u001b[1;32m     23\u001b[0m bi_train_accs\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "Cell \u001b[0;32mIn[104], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(feature_type, model_type, train_exs, reg_lambda, learning_rate)\u001b[0m\n\u001b[1;32m     16\u001b[0m     model \u001b[38;5;241m=\u001b[39m AlwaysPositiveClassifier()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogisticRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_exs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_lambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass AlwaysPositive or LogisticRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[103], line 14\u001b[0m, in \u001b[0;36mtrain_lr\u001b[0;34m(train_exs, feat_extractor, reg_lambda, learning_rate)\u001b[0m\n\u001b[1;32m     11\u001b[0m     feat_extractor\u001b[38;5;241m.\u001b[39mextract_features(train, add_to_indexer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize and return an object of instance LogisticRegressionClassifier\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m log_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegressionClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_exs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_classifier\n",
      "Cell \u001b[0;32mIn[101], line 23\u001b[0m, in \u001b[0;36mLogisticRegressionClassifier.__init__\u001b[0;34m(self, feat_extractor, train_examples, num_iters, reg_lambda, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_extractor \u001b[38;5;241m=\u001b[39m feat_extractor\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# STEP 2: Call the train() function. (This has already been done for you)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m##### SOLUTION START #####\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m##### SOLUTION END #####\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[101], line 53\u001b[0m, in \u001b[0;36mLogisticRegressionClassifier.train\u001b[0;34m(self, feat_extractor, train_examples, num_iters, reg_lambda, learning_rate)\u001b[0m\n\u001b[1;32m     50\u001b[0m     one_hot[input_feature_indexes] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     dw \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_prime\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m reg_lambda \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n\u001b[1;32m     54\u001b[0m     db \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y_prime \u001b[38;5;241m-\u001b[39m y\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Update weights and biases using the mean of the gradients\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Experiment with different regularization parameters and plot train and dev accuracies\n",
    "# You can either hard code the values or,\n",
    "# write a loop that calculates accuracies for different parameters\n",
    "regs = [0.001,0.01, 0.05, 0.1, 0.5, 1, 10]\n",
    "\n",
    "uni_train_accs =[]\n",
    "bi_dev_accs = []\n",
    "\n",
    "bi_train_accs =[]\n",
    "uni_dev_accs = []\n",
    "\n",
    "for r in regs:\n",
    "    print(r)\n",
    "    unigram_model = train_model('unigram', 'LogisticRegression', train_exs,reg_lambda=0.1,learning_rate = r)\n",
    "    train_acc, dev_acc = eval_train_dev(unigram_model)\n",
    "\n",
    "    uni_train_accs.append(train_acc)\n",
    "    uni_dev_accs.append(dev_acc)\n",
    "\n",
    "    bigram_model = train_model('bigram', 'LogisticRegression', train_exs,reg_lambda=0.1, learning_rate = r)\n",
    "    train_acc, dev_acc = eval_train_dev(bigram_model)\n",
    "\n",
    "    bi_train_accs.append(train_acc)\n",
    "    bi_dev_accs.append(dev_acc)\n",
    "\n",
    "\n",
    "\n",
    "### SOLUTION START ###\n",
    "\n",
    "### SOLUTION END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([99.6820809248555,\n",
       "  99.63872832369943,\n",
       "  99.6820809248555,\n",
       "  99.73988439306359,\n",
       "  99.63872832369943,\n",
       "  99.65317919075144,\n",
       "  99.66763005780346,\n",
       "  99.63872832369943],\n",
       " [0.001, 0.01, 0.05, 0.1, 0.5, 1, 10, 100])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_train_accs, regs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "zYQ-Rvt5JVV4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "o_xk3fINHzFy",
    "outputId": "d4ce265b-54f0-49f7-e977-4d5dc46637cf"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log10(regs), uni_train_accs,  label = \"training\")\n",
    "plt.plot(np.log10(regs), uni_dev_accs,  label = \"testing\")\n",
    "plt.title(\"Unigram Accuracy\")\n",
    "plt.xlabel(\"lambda values (log10)\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([99.6820809248555,\n",
       "  99.63872832369943,\n",
       "  99.6820809248555,\n",
       "  99.73988439306359,\n",
       "  99.63872832369943,\n",
       "  99.65317919075144,\n",
       "  99.66763005780346,\n",
       "  99.63872832369943],\n",
       " [77.63761467889908,\n",
       "  77.29357798165138,\n",
       "  77.29357798165138,\n",
       "  76.37614678899082,\n",
       "  77.5229357798165,\n",
       "  77.5229357798165,\n",
       "  77.5229357798165,\n",
       "  77.40825688073394])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_train_accs, uni_dev_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "a86-nnCdHzip",
    "outputId": "5ab7726a-4bea-4dd2-c5ac-3b8d2b253e32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x127f4e350>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log10(regs), bi_train_accs,  label = \"training\")\n",
    "plt.plot(np.log10(regs), bi_dev_accs,  label = \"testing\")\n",
    "plt.title(\"Bigram Accuracy\")\n",
    "plt.xlabel(\"lambda values (log10)\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.98554913294798, 100.0],\n",
       " [64.22018348623853,\n",
       "  64.56422018348624,\n",
       "  64.56422018348624,\n",
       "  64.44954128440367,\n",
       "  63.64678899082569,\n",
       "  63.99082568807339,\n",
       "  64.10550458715596,\n",
       "  63.76146788990825])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_train_accs, bi_dev_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSE 2b:\n",
    "I notice that it doesnt change too much. I expected more of difference  between using the regularizer and not using it. I would imagine since this is high dimensional data that L2 would help increase testing accuracy. Perhaps the features are too sparse for regularizers make a big enough impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Morgan_Classification (COS484 S2022).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
